# -*- coding: utf-8 -*-
"""Data_Cleaning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yrH1jq-K_nOLhDIcQFBbxAidlnZnqfR0
"""

from google.colab import files
uploaded = files.upload()  # Yükleme ekranı açılır, dosyanı buradan seç

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Pandas ve diğer kütüphaneler zaten yüklü olacaktır ama ihtiyaca göre aşağıdaki gibi kurabilirsin
!pip install pandas
!pip install matplotlib
!pip install wordcloud

import pandas as pd

df = pd.read_csv('netflix_data.csv')
df.head()

"""re: Düzenli ifadeler ile metindeki istenmeyen karakterleri (noktalama işaretleri, sayılar vb.) temizlemekte kullanılır.
stopwords: Doğal dilde anlam taşımayan "and", "the", "is" gibi kelimeleri tanımlayan bir liste sağlar.
word_tokenize: Bir metni kelimelere (tokens) ayırmak için kullanılır.
WordNetLemmatizer: Kelimeleri köklerine indirgemek için kullanılır. Örneğin, "running" -> "run" haline gelir.
"""

import re  # Düzenli ifadeler (regular expressions) ile metni işlemek için kullanıyoruz.
from nltk.corpus import stopwords  # NLTK'nin stop words (gereksiz kelimeler) listesini alıyoruz.
from nltk.tokenize import word_tokenize  # Metni kelimelere (tokens) bölmek için kullanıyoruz.
from nltk.stem import WordNetLemmatizer  # Kelimeleri köklerine indirmek (lemmatization) için kullanıyoruz.

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

import re

text = "Hey! Bu bir deneme cümlesi,değil mi? Evet, öyle... Peki, nasıl oldu? Harika!"
cleaned_text = re.sub(r'[, ! ?]', ' ', text)  # Sadece virgülleri siler
cleaned_text = re.sub(r'\s+', ' ', cleaned_text)  # Birden fazla aynı karakteri tek bir karakterle değiştirir
print(cleaned_text)

"""Açıklama:
re.sub(r',', ' ', text): Bu düzenli ifade virgül karakterlerini (,) bulur ve bunları bir boşluk karakteriyle (' ') değiştirir. Diğer noktalama işaretlerine dokunulmaz, sadece virgüller etkilenir.
"""

import re

text = "Hey! Bu bir deneme cümlesi, değil mi? Evet, öyle... Peki, nasıl oldu? Harika!"
cleaned_text = re.sub(r',', ' ', text)  # Sadece virgülleri boşluk ile değiştirir
print(cleaned_text)

import sklearn

!pip install sklearn

from sklearn.feature_extraction.text import TfidfVectorizer

# Örnek temizlenmiş metinler
cleaned_descriptions = ['bu bir örnek cümle', 'örnek cümle ve kelime', 'veri temizleme işlemi']

# TF-IDF vektörizer tanımlıyoruz
vectorizer = TfidfVectorizer()

# Metinleri vektörize ediyoruz
tfidf_matrix = vectorizer.fit_transform(cleaned_descriptions)

# Sonuçları görmek için vektör kelimelerini yazdıralım
print(vectorizer.get_feature_names_out())
print(tfidf_matrix.toarray())

"""Boş (Eksik) Verilerin İşlenmesi
Veri setinde boş (NaN) ya da eksik değerler olabilir. Bu değerleri yönetmek için birkaç strateji uygulanabilir:

Eksik Veriyi Atma: Eğer veri setindeki eksik değerler (NaN) çok fazla değilse, eksik veriyi atanabilirsin.

"""

df_cleaned = df.dropna()  # Tüm satırları temizler, eksik veri içeren satırları çıkarır
print(df_cleaned)

df_cleaned = df.fillna('Unknown')  # Eksik verileri 'Unknown' ile doldurur
print(df_cleaned)

df_cleaned = df[df['duration'] >= 0]  # Negatif yaş değerlerini çıkarır
df_cleaned = df[df['release_year'] >= 0]  # Negatif yaş değerlerini çıkarır
print(df_cleaned)

df_cleaned = df.drop_duplicates()  # Tekrar eden satırları temizler
print(df_cleaned)

df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')
print(df)

df['description'] = df['description'].str.lower()  # Tüm metinleri küçük harfe çevirir
print(df)

"""Büyük/Küçük Harf Dönüşümü: Eğer veri setinde metinsel verilerde büyük/küçük harf tutarsızlığı varsa, tüm veriyi küçük harfe çevirebilirsin."""



"""Stop Words (Gereksiz Kelimeler) Kaldırma
Stop words, metinlerde sıklıkla kullanılan ama anlamsız kelimelerdir. Bunları temizlemek metin analizi için faydalıdır.

NLTK ile Stop Words Temizleme:
"""

from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

df['cleaned_text'] = df['description'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
print(df)

"""Lemmatization veya Stemming
Metinlerdeki kelimeleri kök haline indirgeyerek (Lemmatization) analiz edebilirsin. Bu, aynı kökten gelen farklı kelimeleri (örneğin "running" ve "ran") aynı köke (örneğin "run") indirger.

NLTK ile Lemmatization:
"""

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))
print(df)

"""Tekrar Eden Kelimeleri Kaldırma
Metinlerde gereksiz tekrarlayan kelimeler olabilir. Bu kelimeleri temizleyebilirsin.

Tekrar Eden Kelimeleri Kaldırma:
"""

df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))
print(df)

df.to_csv('cleaned_data.csv', index=False)  # Temizlenmiş veriyi kaydeder
print(df.to_string())

