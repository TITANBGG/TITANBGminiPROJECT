# -*- coding: utf-8 -*-
"""translateapp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f63f82WcnCTZ9OsKSejjsl95DfU5pvqz
"""

# Gerekli kütüphaneleri yükleyelim
!pip install tensorflow requests

from zipfile import ZipFile
import pathlib
import requests

def download_dataset(url, save_path=""):
    """
    URL'den veri indirir ve belirtilen dizine kaydeder
    """
    file_name = url.rsplit("/", maxsplit=1)[1]  # URL'den dosya adını alır
    save_path = pathlib.Path(save_path)
    save_path.mkdir(parents=True, exist_ok=True)
    save_path /= file_name

    # İndirme işlemi
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)

    # Dosyayı kaydet
    with open(save_path, "wb") as file:
        file.write(response.content)

    # Zip dosyasını açma
    with ZipFile(save_path, "r") as compressed:
        compressed.extractall(save_path.parent)

# Örnek kullanım:
# download_dataset("https://example.com/dataset.zip", save_path="datasets")

import numpy as np

def split_dataset(dataset_path, val_ratio=0.15, test_ratio=0.15):
    """
    Veri setini eğitim, doğrulama ve test setlerine böler
    """
    with open(dataset_path, "r") as dataset:
        text_lines = dataset.read().split("\n")[:-1]  # Son satırdaki boşluğu çıkarır

    text_pairs = list()
    for text_line in text_lines:
        eng, tur, *_ = text_line.split("\t")  # İngilizce ve Türkçe cümleleri ayırır
        tur = "[start] " + tur + " [end]"  # Türkçe cümlelere başla ve bitir işaretleri ekler
        text_pairs.append((eng, tur))

    np.random.shuffle(text_pairs)  # Cümle çiftlerini karıştırır

    num_samples = len(text_pairs)
    num_val_samples = int(val_ratio * num_samples)
    num_test_samples = int(test_ratio * num_samples)

    val_pairs = text_pairs[:num_val_samples]
    test_pairs = text_pairs[num_val_samples : num_val_samples + num_test_samples]
    train_pairs = text_pairs[num_val_samples + num_test_samples:]

    return train_pairs, val_pairs, test_pairs

# Örnek kullanım:
# train_pairs, val_pairs, test_pairs = split_dataset("path/to/dataset.txt")

import string
import re
from tensorflow.keras.layers import TextVectorization
import tensorflow as tf

def build_vectorizers(train_pairs, vocab_size, max_length):
    """
    Eğitim verilerinden vektörleştirici oluşturur
    """
    strip_chars = string.punctuation.replace("[", "").replace("]", "")

    def standardization(input_str):
        """
        Standartlaştırma fonksiyonu (büyük/küçük harf farkı ve noktalama temizleme)
        """
        lowercase = tf.strings.lower(input_str)
        return tf.strings.regex_replace(lowercase, f"[{re.escape(strip_chars)}]", "")

    source_vectorizer = TextVectorization(
        max_tokens=vocab_size, output_mode="int", output_sequence_length=max_length
    )

    target_vectorizer = TextVectorization(
        max_tokens=vocab_size,
        output_mode="int",
        output_sequence_length=max_length + 1,
        standardize=standardization,
    )

    english_texts = [pair[0] for pair in train_pairs]
    turkish_texts = [pair[1] for pair in train_pairs]

    source_vectorizer.adapt(english_texts)  # İngilizce cümlelere vektörleştirici uygula
    target_vectorizer.adapt(turkish_texts)  # Türkçe cümlelere vektörleştirici uygula

    return source_vectorizer, target_vectorizer

# Örnek kullanım:
# source_vectorizer, target_vectorizer = build_vectorizers(train_pairs, vocab_size=20000, max_length=100)

def create_dataset(pairs, vectorizers, batch_size=64, num_parallel_calls=None):
    """
    Veri setini eğitim için uygun formata getirir
    """
    source_vectorizer, target_vectorizer = vectorizers

    def format_dataset(eng, tur):
        """
        Eğitim için formatlanan veri seti
        """
        eng = source_vectorizer(eng)
        tur = target_vectorizer(tur)

        return (
            {"english": eng, "turkish": tur[:, :-1]},
            tur[:, 1:],
        )

    eng, tur = zip(*pairs)
    eng = list(eng)
    tur = list(tur)

    dataset = tf.data.Dataset.from_tensor_slices((eng, tur))
    dataset = dataset.batch(batch_size)
    dataset = dataset.map(format_dataset, num_parallel_calls=num_parallel_calls)

    return dataset.shuffle(2048).prefetch(1).cache()

# Örnek kullanım:
# dataset = create_dataset(train_pairs, (source_vectorizer, target_vectorizer))

def create_dataset(pairs, vectorizers, batch_size=64, num_parallel_calls=None):
    """
    Veri setini eğitim için uygun formata getirir
    """
    source_vectorizer, target_vectorizer = vectorizers

    def format_dataset(eng, tur):
        """
        Eğitim için formatlanan veri seti
        """
        eng = source_vectorizer(eng)
        tur = target_vectorizer(tur)

        return (
            {"english": eng, "turkish": tur[:, :-1]},
            tur[:, 1:],
        )

    eng, tur = zip(*pairs)
    eng = list(eng)
    tur = list(tur)

    dataset = tf.data.Dataset.from_tensor_slices((eng, tur))
    dataset = dataset.batch(batch_size)
    dataset = dataset.map(format_dataset, num_parallel_calls=num_parallel_calls)

    return dataset.shuffle(2048).prefetch(1).cache()

# Örnek kullanım:
# dataset = create_dataset(train_pairs, (source_vectorizer, target_vectorizer))

from tensorflow.keras import layers

def create_seq2seq_model(vocab_size, embedding_dim, rnn_units, batch_size, max_length):
    """
    Basit bir Seq2Seq modeli kurar

    Parameters
    ----------
    vocab_size : int
                 Kaynak ve hedef diller için kelime sayısı

    embedding_dim : int
                   Embedding katmanı için boyut

    rnn_units : int
                RNN (LSTM) katmanı için birim sayısı

    batch_size : int
                 Modelin batch boyutu

    max_length : int
                 Kaynak ve hedef cümlelerin maksimum uzunluğu

    Returns
    -------
    model : tf.keras.Model
            Seq2Seq modeli
    """
    # Encoder kısmı (girdi dilini anlamak için)
    encoder_inputs = layers.Input(shape=(max_length,), name="english")
    encoder_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)
    encoder_outputs, state_h, state_c = layers.LSTM(units=rnn_units, return_state=True)(encoder_embedding)

    # Decoder kısmı (hedef dilde çeviri üretmek için)
    decoder_inputs = layers.Input(shape=(max_length,), name="turkish")
    decoder_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)
    decoder_lstm = layers.LSTM(units=rnn_units, return_sequences=True)(decoder_embedding, initial_state=[state_h, state_c])
    decoder_outputs = layers.Dense(vocab_size, activation="softmax")(decoder_lstm)

    # Modeli birleştirme
    model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)
    return model

# Örnek model kurulum
vocab_size = 20000  # Kelime sayısı
embedding_dim = 256  # Embedding boyutu
rnn_units = 512  # LSTM birimleri
max_length = 100  # Maksimum cümle uzunluğu
batch_size = 64

seq2seq_model = create_seq2seq_model(vocab_size, embedding_dim, rnn_units, batch_size, max_length)
seq2seq_model.summary()  # Model özetini görüntüle

seq2seq_model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

from tensorflow.keras import layers

def create_seq2seq_model(vocab_size, embedding_dim, rnn_units, batch_size, max_length):
    """
    Basit bir Seq2Seq modeli kurar

    Parameters
    ----------
    vocab_size : int
                 Kaynak ve hedef diller için kelime sayısı

    embedding_dim : int
                   Embedding katmanı için boyut

    rnn_units : int
                RNN (LSTM) katmanı için birim sayısı

    batch_size : int
                 Modelin batch boyutu

    max_length : int
                 Kaynak ve hedef cümlelerin maksimum uzunluğu

    Returns
    -------
    model : tf.keras.Model
            Seq2Seq modeli
    """
    # Encoder kısmı (girdi dilini anlamak için)
    encoder_inputs = layers.Input(shape=(max_length,), name="english")
    encoder_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)
    encoder_outputs, state_h, state_c = layers.LSTM(units=rnn_units, return_state=True)(encoder_embedding)

    # Decoder kısmı (hedef dilde çeviri üretmek için)
    decoder_inputs = layers.Input(shape=(max_length,), name="turkish")
    decoder_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)
    decoder_lstm = layers.LSTM(units=rnn_units, return_sequences=True)(decoder_embedding, initial_state=[state_h, state_c])
    decoder_outputs = layers.Dense(vocab_size, activation="softmax")(decoder_lstm)

    # Modeli birleştirme
    model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)
    return model

# Örnek model kurulum
vocab_size = 20000  # Kelime sayısı
embedding_dim = 256  # Embedding boyutu
rnn_units = 512  # LSTM birimleri
max_length = 100  # Maksimum cümle uzunluğu
batch_size = 64

seq2seq_model = create_seq2seq_model(vocab_size, embedding_dim, rnn_units, batch_size, max_length)
seq2seq_model.summary()  # Model özetini görüntüle

seq2seq_model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

import pandas as pd

from google.colab import files

# Dosyaları yükle
uploaded = files.upload()

import pandas as pd

# Yüklenen dosyaları pandas ile okuma
df1 = pd.read_excel("tur.xls")
df2 = pd.read_excel("tur 2.xlsx")

# Dosyaların ilk 5 satırını kontrol edelim
print("tur.xls ilk 5 satır:")
print(df1.head())

print("\ntur2.xlsx ilk 5 satır:")
print(df2.head())

test_loss, test_acc = seq2seq_model.evaluate(test_dataset)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")

def translate_sentence(model, source_vectorizer, target_vectorizer, sentence, max_length):
    """
    Eğitilen modeli kullanarak bir cümleyi çevirir
    """
    sentence_vectorized = source_vectorizer([sentence])
    encoder_input = tf.convert_to_tensor(sentence_vectorized)

    # Encoder'in çıktısını al
    encoder_output, state_h, state_c = model.layers[2](encoder_input)

    # Decoder'e başlangıç olarak "[start]" token'ı veriyoruz
    start_token = target_vectorizer(["[start]"])[0]
    decoder_input = tf.convert_to_tensor([[start_token]])

    translated_sentence = []

    # Cümleyi kelime kelime üretme döngüsü
    for _ in range(max_length):
        decoder_output, state_h, state_c = model.layers[3](decoder_input, initial_state=[state_h, state_c])
        predicted_id = tf.argmax(decoder_output[0, -1, :], axis=-1).numpy()

        # Tahmin edilen token'ı çözüp cümleye ekleme
        predicted_word = target_vectorizer.get_vocabulary()[predicted_id]
        if predicted_word == "[end]":
            break
        translated_sentence.append(predicted_word)

        # Bir sonraki kelime için decoder girişini ayarlama
        decoder_input = tf.expand_dims([predicted_id], axis=0)

    return " ".join(translated_sentence)

# Örnek kullanım
sentence = "Hello, how are you?"
translated = translate_sentence(seq2seq_model, source_vectorizer, target_vectorizer, sentence, max_length=100)
print(f"Translated Sentence: {translated}")

